{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3MKmpqdEuZv"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofd3p0cPExJg",
        "outputId": "a64e2fea-bb13-4235-88aa-42368458e265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FoT2SopE0Au"
      },
      "outputs": [],
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"EECS_595_NLP/Project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va7hoW7rE1Hc",
        "outputId": "5e07b6a6-9687-4096-8bd5-c2f3f2e69d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['glove.6B.200d.txt', 'glove.twitter.27B.200d.txt', 'Nov 6 note.txt', 'Embedding.py', 'glove', 'en-ids.tsv', 'en_2013_01_01.pkl', 'rehydrated_tweets', 'rehydrate_2013_01_multiprocessing.py', 'EmoTag-Vectors-620d', 'Embedding.ipynb']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "assert sys.version_info[0] == 3\n",
        "assert sys.version_info[1] >= 5\n",
        "\n",
        "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "assert \"Embedding.ipynb\" in os.listdir(GOOGLE_DRIVE_PATH), \"<Warning>: Embedding.ipynb not found.\"\n",
        "assert \"Embedding.py\" in os.listdir(\n",
        "    GOOGLE_DRIVE_PATH\n",
        "), \"<Warning>: Embedding not found.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB-cJNaGFOFV",
        "outputId": "e3a44c37-41fc-492e-e7bc-3f8e82311d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/EECS_595_NLP\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/EECS_595_NLP/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VOOkHb7Jf73",
        "outputId": "3bd08f11-d173-434f-93e6-7dfa9635747f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnU6LsoTUVMP"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1VAY0qUYGH"
      },
      "source": [
        "Before training, please manually  chage parser train value as True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptBeZldmEpXa",
        "outputId": "84d5b4a5-a612-4f8e-e594-797928c298e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##############################\n",
            "get counts of each word in dic\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 180238.66it/s]\n",
            "##############################\n",
            "get UNKA words and replace them in words_by_sentence\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 319781.09it/s]\n",
            "##############################\n",
            "get unique tags and place in dict with index as values\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 366881.13it/s]\n",
            "##############################\n",
            "create vocabulary dictionary with index as values\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 315749.87it/s]\n",
            "##############################\n",
            "convert words and tags by sentence to their index\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 189849.97it/s]\n",
            "100% 38219/38219 [00:00<00:00, 264271.56it/s]\n",
            "##############################\n",
            "create batches for words and tags\n",
            "##############################\n",
            "100% 383/383 [00:00<00:00, 14033.04it/s]\n",
            "##############################\n",
            "pad each batch for words and tags\n",
            "##############################\n",
            "100% 383/383 [00:00<00:00, 2132.86it/s]\n",
            "##############################\n",
            "Load Pre-trained Embedding\n",
            "##############################\n",
            "100% 401030/401030 [01:55<00:00, 3460.28it/s]\n",
            "##############################\n",
            "create weight matrix of pre-trained embeddings\n",
            "##############################\n",
            "Embedding dimension : 620\n",
            "16926it [00:00, 31812.58it/s]\n",
            "vocab_tag_weight saved\n",
            "##############################\n",
            "Train Model\n",
            "##############################\n",
            "  0% 0/30 [00:00<?, ?it/s]rnnpos.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  padded_tag_batch = torch.LongTensor(padded_tag_batch).view(-1)\n",
            "FINISHED EPOCH: 1\n",
            "  3% 1/30 [00:44<21:26, 44.35s/it]FINISHED EPOCH: 2\n",
            "  7% 2/30 [01:29<20:49, 44.61s/it]FINISHED EPOCH: 3\n",
            " 10% 3/30 [02:13<20:06, 44.69s/it]FINISHED EPOCH: 4\n",
            " 13% 4/30 [02:58<19:24, 44.79s/it]FINISHED EPOCH: 5\n",
            " 17% 5/30 [03:43<18:37, 44.70s/it]FINISHED EPOCH: 6\n",
            " 20% 6/30 [04:28<17:53, 44.71s/it]FINISHED EPOCH: 7\n",
            " 23% 7/30 [05:14<17:22, 45.32s/it]FINISHED EPOCH: 8\n",
            " 27% 8/30 [06:00<16:39, 45.45s/it]FINISHED EPOCH: 9\n",
            " 30% 9/30 [06:46<15:58, 45.64s/it]FINISHED EPOCH: 10\n",
            " 33% 10/30 [07:32<15:15, 45.77s/it]FINISHED EPOCH: 11\n",
            " 37% 11/30 [08:18<14:33, 45.96s/it]FINISHED EPOCH: 12\n",
            " 40% 12/30 [09:05<13:49, 46.08s/it]FINISHED EPOCH: 13\n",
            " 43% 13/30 [09:53<13:12, 46.60s/it]FINISHED EPOCH: 14\n",
            " 47% 14/30 [10:40<12:30, 46.89s/it]FINISHED EPOCH: 15\n",
            " 50% 15/30 [11:28<11:48, 47.21s/it]FINISHED EPOCH: 16\n",
            " 53% 16/30 [12:16<11:03, 47.42s/it]FINISHED EPOCH: 17\n",
            " 57% 17/30 [13:04<10:17, 47.46s/it]FINISHED EPOCH: 18\n",
            " 60% 18/30 [13:52<09:31, 47.63s/it]FINISHED EPOCH: 19\n",
            " 63% 19/30 [14:39<08:43, 47.60s/it]FINISHED EPOCH: 20\n",
            " 67% 20/30 [15:27<07:57, 47.74s/it]FINISHED EPOCH: 21\n",
            " 70% 21/30 [16:15<07:09, 47.71s/it]FINISHED EPOCH: 22\n",
            " 73% 22/30 [17:03<06:22, 47.81s/it]FINISHED EPOCH: 23\n",
            " 77% 23/30 [17:51<05:34, 47.80s/it]FINISHED EPOCH: 24\n",
            " 80% 24/30 [18:39<04:47, 47.92s/it]FINISHED EPOCH: 25\n",
            " 83% 25/30 [19:28<04:00, 48.18s/it]FINISHED EPOCH: 26\n",
            " 87% 26/30 [20:17<03:13, 48.46s/it]FINISHED EPOCH: 27\n",
            " 90% 27/30 [21:06<02:26, 48.84s/it]FINISHED EPOCH: 28\n",
            " 93% 28/30 [21:56<01:37, 48.98s/it]FINISHED EPOCH: 29\n",
            " 97% 29/30 [22:46<00:49, 49.23s/it]FINISHED EPOCH: 30\n",
            "100% 30/30 [23:35<00:00, 47.19s/it]\n"
          ]
        }
      ],
      "source": [
        "#'glove200d', 'glovetwit200d', 'Emotag'\n",
        "# how to use --train flag ? \n",
        "!python rnnpos.py --data_file=HW2/wsj19-21.testing --label_file=HW2/wsj19-21.truth --model_file=HW2/model.torch --training_file=HW2/wsj1-18.training --pre_trained_embedding_file=Emotag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROvRWfCVNhZj"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HjYFYmqUenY"
      },
      "source": [
        "Before training, please manually  chage parser train value as False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcGPEhqINgsB",
        "outputId": "f03707d0-6166-43f7-86bf-5e4407e4f659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1247338496 bytes == 0x5da2e000 @  0x7f0a7a48eb6b 0x7f0a7a4ae379 0x7f0a0a9cbd57 0x7f0a0a9b9bc3 0x7f0a34d3c560 0x7f0a34d3ca85 0x7f0a34d3da20 0x7f0a34db1007 0x7f0a34db3126 0x7f0a3564a1f3 0x7f0a3519d36b 0x7f0a34b61248 0x7f0a3579a72f 0x7f0a35580733 0x7f0a3695d2ae 0x7f0a3695d9a6 0x7f0a355bea6d 0x7f0a5c61d3bb 0x58f6e4 0x5105e2 0x5b4ee6 0x58ff2e 0x510325 0x4bac0a 0x4d3249 0x591e56 0x50e18c 0x5b4ee6 0x4bad99 0x538a76 0x590ae5\n",
            "tcmalloc: large alloc 1247338496 bytes == 0xa7fd4000 @  0x7f0a7a48eb6b 0x7f0a7a4ae379 0x7f0a0a9cbd57 0x7f0a0a9b9bc3 0x7f0a34d3c560 0x7f0a34d3ca85 0x7f0a34d3da20 0x7f0a34db1007 0x7f0a34db3126 0x7f0a3564a1f3 0x7f0a3511ea4c 0x7f0a366ba89c 0x7f0a366baea6 0x7f0a3519d36b 0x7f0a5c57f2e2 0x4d23e0 0x51041f 0x5b4ee6 0x58ff2e 0x50d482 0x4bac0a 0x4d3249 0x591e56 0x50e18c 0x5b4ee6 0x4bad99 0x538a76 0x590ae5 0x510280 0x58fd37 0x50c4fc\n",
            "The accuracy of the model is  90.75%\n"
          ]
        }
      ],
      "source": [
        "!python rnnpos.py --data_file=HW2/wsj19-21.testing --label_file=HW2/wsj19-21.truth --model_file=HW2/model.torch --training_file=HW2/wsj1-18.training --pre_trained_embedding_file=glove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv9RpqpcUtTd"
      },
      "source": [
        "# Glove embedding Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYf8B5TjVDIZ",
        "outputId": "812751bd-bbf3-4ea6-95c1-23c6e13e7175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##############################\n",
            "get counts of each word in dic\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 206667.02it/s]\n",
            "##############################\n",
            "get UNKA words and replace them in words_by_sentence\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 333864.65it/s]\n",
            "##############################\n",
            "get unique tags and place in dict with index as values\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 428984.59it/s]\n",
            "##############################\n",
            "create vocabulary dictionary with index as values\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 293689.88it/s]\n",
            "##############################\n",
            "convert words and tags by sentence to their index\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 210693.72it/s]\n",
            "100% 38219/38219 [00:00<00:00, 309826.58it/s]\n",
            "##############################\n",
            "create batches for words and tags\n",
            "##############################\n",
            "100% 383/383 [00:00<00:00, 14796.02it/s]\n",
            "##############################\n",
            "pad each batch for words and tags\n",
            "##############################\n",
            "100% 383/383 [00:00<00:00, 2193.72it/s]\n",
            "##############################\n",
            "Load Pre-trained Embedding\n",
            "##############################\n",
            "100% 400000/400000 [00:20<00:00, 19407.75it/s]\n",
            "##############################\n",
            "create weight matrix of pre-trained embeddings\n",
            "##############################\n",
            "Embedding dimension : 200\n",
            "16926it [00:00, 83481.94it/s]\n",
            "vocab_tag_weight saved\n",
            "##############################\n",
            "Train Model\n",
            "##############################\n",
            "  0% 0/30 [00:00<?, ?it/s]rnnpos.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  padded_tag_batch = torch.LongTensor(padded_tag_batch).view(-1)\n",
            "FINISHED EPOCH: 1\n",
            "  3% 1/30 [00:36<17:50, 36.92s/it]FINISHED EPOCH: 2\n",
            "  7% 2/30 [01:14<17:28, 37.44s/it]FINISHED EPOCH: 3\n",
            " 10% 3/30 [01:51<16:48, 37.36s/it]FINISHED EPOCH: 4\n",
            " 13% 4/30 [02:30<16:21, 37.75s/it]FINISHED EPOCH: 5\n",
            " 17% 5/30 [03:08<15:49, 37.98s/it]FINISHED EPOCH: 6\n",
            " 20% 6/30 [03:46<15:11, 37.98s/it]FINISHED EPOCH: 7\n",
            " 23% 7/30 [04:25<14:36, 38.10s/it]FINISHED EPOCH: 8\n",
            " 27% 8/30 [05:03<13:57, 38.07s/it]FINISHED EPOCH: 9\n",
            " 30% 9/30 [05:40<13:18, 38.02s/it]FINISHED EPOCH: 10\n",
            " 33% 10/30 [06:18<12:35, 37.76s/it]FINISHED EPOCH: 11\n",
            " 37% 11/30 [06:55<11:56, 37.72s/it]FINISHED EPOCH: 12\n",
            " 40% 12/30 [07:34<11:23, 37.94s/it]FINISHED EPOCH: 13\n",
            " 43% 13/30 [08:12<10:48, 38.12s/it]FINISHED EPOCH: 14\n",
            " 47% 14/30 [08:51<10:12, 38.29s/it]FINISHED EPOCH: 15\n",
            " 50% 15/30 [09:30<09:36, 38.44s/it]FINISHED EPOCH: 16\n",
            " 53% 16/30 [10:09<08:59, 38.55s/it]FINISHED EPOCH: 17\n",
            " 57% 17/30 [10:47<08:21, 38.57s/it]FINISHED EPOCH: 18\n",
            " 60% 18/30 [11:26<07:44, 38.72s/it]FINISHED EPOCH: 19\n",
            " 63% 19/30 [12:05<07:05, 38.66s/it]FINISHED EPOCH: 20\n",
            " 67% 20/30 [12:43<06:26, 38.68s/it]FINISHED EPOCH: 21\n",
            " 70% 21/30 [13:22<05:47, 38.64s/it]FINISHED EPOCH: 22\n",
            " 73% 22/30 [14:01<05:09, 38.63s/it]FINISHED EPOCH: 23\n",
            " 77% 23/30 [14:39<04:30, 38.70s/it]FINISHED EPOCH: 24\n",
            " 80% 24/30 [15:18<03:52, 38.67s/it]FINISHED EPOCH: 25\n",
            " 83% 25/30 [15:57<03:13, 38.70s/it]FINISHED EPOCH: 26\n",
            " 87% 26/30 [16:36<02:35, 38.75s/it]FINISHED EPOCH: 27\n",
            " 90% 27/30 [17:15<01:56, 38.86s/it]FINISHED EPOCH: 28\n",
            " 93% 28/30 [17:54<01:17, 38.90s/it]FINISHED EPOCH: 29\n",
            " 97% 29/30 [18:33<00:38, 38.97s/it]FINISHED EPOCH: 30\n",
            "100% 30/30 [19:12<00:00, 38.42s/it]\n",
            "The accuracy of the model is  90.96%\n"
          ]
        }
      ],
      "source": [
        "!python rnnpos.py --data_file=HW2/wsj19-21.testing --label_file=HW2/wsj19-21.truth --model_file=HW2/model.torch --training_file=HW2/wsj1-18.training --pre_trained_embedding_file=glove200d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NIg2mw3VFpw"
      },
      "source": [
        "# Glove twit 200d train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3j_N9rEZVH5T",
        "outputId": "b15fa16c-a2a4-4c90-b6ba-94bd5a3b5a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##############################\n",
            "get counts of each word in dic\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 208356.65it/s]\n",
            "##############################\n",
            "get UNKA words and replace them in words_by_sentence\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 328313.00it/s]\n",
            "##############################\n",
            "get unique tags and place in dict with index as values\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 433333.26it/s]\n",
            "##############################\n",
            "create vocabulary dictionary with index as values\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 328511.48it/s]\n",
            "##############################\n",
            "convert words and tags by sentence to their index\n",
            "##############################\n",
            "100% 38219/38219 [00:00<00:00, 220495.05it/s]\n",
            "100% 38219/38219 [00:00<00:00, 315585.14it/s]\n",
            "##############################\n",
            "create batches for words and tags\n",
            "##############################\n",
            "100% 383/383 [00:00<00:00, 14879.62it/s]\n",
            "##############################\n",
            "pad each batch for words and tags\n",
            "##############################\n",
            "100% 383/383 [00:00<00:00, 2196.63it/s]\n",
            "##############################\n",
            "Load Pre-trained Embedding\n",
            "##############################\n",
            "100% 1193514/1193514 [01:03<00:00, 18740.92it/s]\n",
            "##############################\n",
            "create weight matrix of pre-trained embeddings\n",
            "##############################\n",
            "Embedding dimension : 200\n",
            "16926it [00:00, 79430.27it/s]\n",
            "vocab_tag_weight saved\n",
            "##############################\n",
            "Train Model\n",
            "##############################\n",
            "  0% 0/30 [00:00<?, ?it/s]rnnpos.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  padded_tag_batch = torch.LongTensor(padded_tag_batch).view(-1)\n",
            "FINISHED EPOCH: 1\n",
            "  3% 1/30 [00:36<17:47, 36.81s/it]FINISHED EPOCH: 2\n",
            "  7% 2/30 [01:14<17:19, 37.12s/it]FINISHED EPOCH: 3\n",
            " 10% 3/30 [01:50<16:37, 36.95s/it]FINISHED EPOCH: 4\n",
            " 13% 4/30 [02:27<15:58, 36.87s/it]FINISHED EPOCH: 5\n",
            " 17% 5/30 [03:04<15:24, 36.99s/it]FINISHED EPOCH: 6\n",
            " 20% 6/30 [03:41<14:48, 37.03s/it]FINISHED EPOCH: 7\n",
            " 23% 7/30 [04:19<14:13, 37.12s/it]FINISHED EPOCH: 8\n",
            " 27% 8/30 [04:56<13:35, 37.06s/it]FINISHED EPOCH: 9\n",
            " 30% 9/30 [05:33<13:00, 37.14s/it]FINISHED EPOCH: 10\n",
            " 33% 10/30 [06:10<12:23, 37.16s/it]FINISHED EPOCH: 11\n",
            " 37% 11/30 [06:48<11:48, 37.29s/it]FINISHED EPOCH: 12\n",
            " 40% 12/30 [07:25<11:12, 37.33s/it]FINISHED EPOCH: 13\n",
            " 43% 13/30 [08:03<10:38, 37.55s/it]FINISHED EPOCH: 14\n",
            " 47% 14/30 [08:41<10:01, 37.58s/it]FINISHED EPOCH: 15\n",
            " 50% 15/30 [09:19<09:24, 37.66s/it]FINISHED EPOCH: 16\n",
            " 53% 16/30 [09:57<08:48, 37.73s/it]FINISHED EPOCH: 17\n",
            " 57% 17/30 [10:35<08:11, 37.81s/it]FINISHED EPOCH: 18\n",
            " 60% 18/30 [11:12<07:33, 37.77s/it]FINISHED EPOCH: 19\n",
            " 63% 19/30 [11:50<06:56, 37.83s/it]FINISHED EPOCH: 20\n",
            " 67% 20/30 [12:28<06:18, 37.81s/it]FINISHED EPOCH: 21\n",
            " 70% 21/30 [13:06<05:40, 37.80s/it]FINISHED EPOCH: 22\n",
            " 73% 22/30 [13:44<05:02, 37.87s/it]FINISHED EPOCH: 23\n",
            " 77% 23/30 [14:22<04:25, 38.00s/it]FINISHED EPOCH: 24\n",
            " 80% 24/30 [15:00<03:47, 38.00s/it]FINISHED EPOCH: 25\n",
            " 83% 25/30 [15:38<03:09, 38.00s/it]FINISHED EPOCH: 26\n",
            " 87% 26/30 [16:16<02:31, 37.85s/it]FINISHED EPOCH: 27\n",
            " 90% 27/30 [16:53<01:53, 37.83s/it]FINISHED EPOCH: 28\n",
            " 93% 28/30 [17:31<01:15, 37.64s/it]FINISHED EPOCH: 29\n",
            " 97% 29/30 [18:08<00:37, 37.49s/it]FINISHED EPOCH: 30\n",
            "100% 30/30 [18:46<00:00, 37.55s/it]\n",
            "The accuracy of the model is  91.94%\n"
          ]
        }
      ],
      "source": [
        "!python rnnpos.py --data_file=HW2/wsj19-21.testing --label_file=HW2/wsj19-21.truth --model_file=HW2/model.torch --training_file=HW2/wsj1-18.training --pre_trained_embedding_file=glovetwit200d"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}